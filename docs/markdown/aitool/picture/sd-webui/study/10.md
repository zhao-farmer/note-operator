<script setup>
import PromptTemplate from '../prompt-template.vue'
</script>

<style scoped src="../prompt-show.css"></style>



# 第十章：ControlNet

## 10.1 基本原理

ControlNet 是一个用于控制 AI 图像生成的插件。它使用了一种称为"Conditional Generative Adversarial Networks"（条件生成对抗网络）的技术来生成图像。与传统的生成对抗网络不同，ControlNet 允许用户对生成的图像进行精细的控制。这使得 ControlNet 在许多应用场景中非常有用，例如计算机视觉、艺术设计、虚拟现实等等。

controlNet 出现之后，我们就能通过模型精准的控制图像生成，比如：上传线稿让 AI 帮我们填色渲染，控制人物的姿态、图片生成线稿等等。

![](/aitool/picture/sd-webui/study/215.png)

如果要画一幅画，一方面是构图，一方面是风格。controlNet就是用来控制构图的，LoRA就是用来控制风格的 。

找到人物的各个要素的关键点，传递给ControlNet，用于控制姿势。

![](/aitool/picture/sd-webui/study/214.png)


## 10.2 安装方式
### 10.2.1 下载插件

1. 方法一：网址安装
    1. 打开“扩展”选项卡。
    2. 在选项卡中打开“从 URL 安装”选项卡。
    3. 输入https://github.com/Mikubill/sd-webui-controlnet.git“扩展的 git 存储库的 URL”。
    4. 按“安装”按钮。
    5. 等待 5 秒钟，您将看到消息“已安装到 stable-diffusion-webui\extensions\sd-webui-controlnet。使用已安装选项卡重新启动”。
    6. 转到“已安装”选项卡，单击“检查更新”，然后单击“应用并重新启动 UI”。（下次您还可以使用这些按钮来更新 ControlNet。）
    7. 完全重新启动 A1111 webui，包括您的终端。（如果您不知道什么是“终端”，可以重新启动计算机以达到相同的效果。）
    8. 下载模型。
    9. 将模型放入正确的文件夹后，您可能需要刷新才能看到模型。刷新按钮位于“模型”下拉列表的右侧。
    ![](/aitool/picture/sd-webui/study/216.png)

2. 方法二：直接下载zip压缩包，解压到“stable-diffusion-webui\extensions\”文件夹中
    1. 下载
    ![](/aitool/picture/sd-webui/study/217.png)
    2. 放入文件夹
    ![](/aitool/picture/sd-webui/study/218.png)
    3. 安装成功后查看
    ![](/aitool/picture/sd-webui/study/219.png)

### 10.2.2 下载模型

1. 模型的分类

    - 从版本来分
    
        可以分为SD1.4、SD1.5和SDXL三个版本的模型
        
        从模型的名称我们可以很容易区分这一点，带“sd14”的是SD1.4版本的模型，带“sd15”的是SD1.5版本的模型，带“xl”的是SDXL版本的模型；
    
    - 从发布机构来分
    
        可以分为官方模型和第三方模型
        
        例如，“T2I-Adapter”系列的模型（包括SD1.5版和SDXL版）是由腾讯ARC实验室所发布的；“qrcode”模型是由monster实验室所发布的；“brightness/illumination”模型是由ioc实验室所发布的；

2. 模型的存储位置

    模型的存储位置有如下两处：
    - "\stable-diffusion-webui\extensions\sd-webui-controlnet\models"
    - "\stable-diffusion-webui\extensions\sd-webui-controlnet\annotator\downloads"

    其中"\stable-diffusion-webui\extensions\sd-webui-controlnet\models"中的模型也可以保存在"\stable-diffusion-webui\models\ControlNet"这个位置，保存在这两处的效果是一样的，都可以被系统自动识别，不过为了统一管理，我强烈建议选择"\stable-diffusion-webui\extensions\sd-webui-controlnet\models"路径，这样对迁移或备份controlNet插件更便捷，只需要复制整个“sd-webui-controlnet”文件夹即可！
    
    "\stable-diffusion-webui\extensions\sd-webui-controlnet\models"中的模型是需要我们手动下载的；

3. 存储信息整理

- 下载完所有模型的controlNet插件总体积达69G

![](/aitool/picture/sd-webui/study/220.png)

- models文件夹中的模型（支持子文件夹）

![](/aitool/picture/sd-webui/study/221.png)

- 子文件夹`sd_control_collection`中的SDXL版本的模型

![](/aitool/picture/sd-webui/study/222.png)

- 子文件夹`T2I-Adapter`中的模型

![](/aitool/picture/sd-webui/study/223.png)

- `annotator/downloads`文件夹中的模型

[](/aitool/picture/sd-webui/study/224.png)

### 10.2.3 模型的下载

- 模型下载页（官方）
    - controlNet模型下载页面：https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main
    - controlNet的qrcode_monster模型下载页面：https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster/tree/main
    - controlNet的brightness和illumination模型下载页面：https://huggingface.co/ioclab/ioc-controlnet/tree/main/models
    - controlNet的SDXL模型下载页面：https://huggingface.co/lllyasviel/sd_control_collection/tree/main
    - controlNet的T2I-Adapter模型下载页面：https://huggingface.co/TencentARC/T2I-Adapter/tree/main/models
    - controlNet的T2I-Adapter模型的SDXL版下载页面：https://huggingface.co/TencentARC/T2I-Adapter/tree/main/models_XL

- 模型下载（网盘下载，中国大陆境内用户可以从下面的网盘链接进行下载！）
    - downloads文件夹下的所有模型：[downloads](https://pan.quark.cn/s/977a913beb71#/list/share)
    安装方法：下载"downloads"文件夹，替换掉"\stable-diffusion-webui\extensions\sd-webui-controlnet\annotator\downloads"文件夹
    models文件夹下的所有模型：[models](https://pan.quark.cn/s/babc3acfd619#/list/share)
    安装方法：下载"models"文件夹，替换掉"\stable-diffusion-webui\extensions\sd-webui-controlnet\models"文件夹

## 10.3 基本使用方式


### 10.3.1 基本流程

1. 上传图片
2. 选择控制类型
3. 选择对应的模型

### 10.3.2 使用OpenPose做案例

1. 上传原图

![](/aitool/picture/sd-webui/study/225.png)

2. ControlNet 配置

![](/aitool/picture/sd-webui/study/226.png)


3. ControlNet 姿势照片

![](/aitool/picture/sd-webui/study/227.png)


4. 提示词

<PromptTemplate>
    <template v-slot:content>
       <span>
        (sfw:1.2),absurdres,1girl,ocean,white dress,long sleeves,sun hat,smile,
       </span>
    </template>
</PromptTemplate>

5. 反向词

<PromptTemplate>
    <template v-slot:content>
       <span>
        (nsfw:1.2),(worst quality:1.2),(low quality:1.2),(lowres:1.1),(monochrome:1.1),(greyscale),multiple views,comic,sketch,animal ears,pointy ears,blurry,transparent,see-through,cleavage,
       </span>
    </template>
</PromptTemplate>

**生成图1**

![](/aitool/picture/sd-webui/study/228.png)

**生成图2**

![](/aitool/picture/sd-webui/study/229.png)


## 10.4 参数详细解析

1. 启用（Enable）

    勾选此选项后，点击 “生成” 按钮时，ControlNet 才会生效。

2. 低显存优化（Low VRAM）

    低显存模式，如果你的显卡内存小于等于4GB，建议勾选此选项。

3. 完美像素模式（Pixed Perfect）

    这个功能是让ControlNet自适应预处理分辨率（Preprocessor resolution）的，开启后会隐藏 （Preprocessor resolution）滑动条

4. 允许预览（Allow Preview）

    启用预览后，用户可以在不渲染整个图像的情况下，实时地看到ControlNet对图像进行操作的效果。
    
    - 第一步 选中允许预览

    ![](/aitool/picture/sd-webui/study/230.png)


    - 第二步 选择控制类型/预处理器，点击爆炸按钮进行生成
    
    ![](/aitool/picture/sd-webui/study/231.png)
    
    - 第三步 对图像进行操作
        - 下载json文件
        - 编辑具体的姿势
        - 进行上传预览

    ![](/aitool/picture/sd-webui/study/232.png)

5. 控制类型（为了选择不同的预处理器）

    跟下方的预处理器与ControlNet模型关联，为想要什么的效果提供不同的解决方案

6. 预处理器（Preprocessor）

    在此列表我们可选择需要的预处理器，每个 ControlNet 的预处理器都有不同的功能，后续将会详细介绍。

7. 模型（Model）
    
    配套各预处理器需要的专属模型。该列表内的模型必须与预处理选项框内的名称选择一致，才能保证正确生成预期结果。如果预处理与模型不致其实也可以出图，但效果无法预料，且一般效果并不理想。
    
8. 权重（Weight）

    权重，代表使用 ControlNet 生成图片时被应用的权重占比。一般都是1
    
9. 引导介入时机（Guidance Start(T)）

    在理解此功能之前，我们应该先知道生成图片的 Sampling steps 采样步数功能，步数代表生成一张图片要刷新计算多少次，Guidance Start(T) 设置为 0 即代表开始时就介入，默认为 0，设置为 0.5 时即代表 ControlNet 从 50% 步数时开始介入计算。
    
10. 引导退出时机（Guidance End(T)）
    
    和引导介入时机相对应，如设置为1，则表示在100%计算完时才会退出介入也就是不退出，默认为 1，可调节范围 0-1，如设置为 0.8 时即代表从80% 步数时退出介入。
    
11. 控制模式

    ControlNet与提示词共同去生成图片，如果两者之间对于生成图拥有不同的权重，这里就是三种模式
    -  均衡 
    -  更偏向提示词
    -  更偏向 ControlNet

12. 缩放模式（Resize Mode）

    用于选择调整图像大小的模式：默认使用（Scale to Fit (Inner Fit)）缩放至合适即可，将会自动适配图片。一共三个选项：
    - 仅调整大小 `Just Resize`
    - 裁剪后缩放 `Scale to Fit (Inner Fit)`
    - 缩放后填充空白 `Envelope (Outer Fit)`

0. 暂时不知道用处的

    ![](/aitool/picture/sd-webui/study/233.png)


## 10.5 ControlNet 模型的原理与应用

- ControlNet控制类型

![](/aitool/picture/sd-webui/study/234.png)

- 使用的原图

![](/aitool/picture/sd-webui/study/235.png)

### 10.5.1 Canny 边缘检测

**1. 原理与配置介绍**

Canny 边缘检测预处理器可很好识别出图像内各对象的边缘轮廓，常用于生成线稿。
- 预处理器介绍
    - canny处理正常的图片
    - invert处理黑白照片
- 预处理器分辨率

    预处理器分辨率数值越高越精细，也越吃显存。但如果数值太低生成的线条也会很粗糙。默认512，具体设置时需根据素材大小和实际情况来做权衡。

- 长和宽的阈值
    
    这个值越高线条越简单，越低线条越复杂。



**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/236.png)

**生成图**

![](/aitool/picture/sd-webui/study/237.png)

### 10.5.2 Depth 深度检测

**1. 原理与配置介绍**

通过提取原始图片中的深度信息，生成具有原图同样深度结构的深度图，越白的越靠前，越黑的越靠后。

在生成的深度图中，不同的灰阶色值表示不同的距离。例如，灰阶数值0的区域表示图像中最远的区域，而灰阶数值255的区域表示图像中最近的区域。因此，通过观察深度图，用户可以更好地理解图像中各个元素之间的距离关系。
ControlNet提供了以下几个预处理器模型，我们来简单看一下每个模型的区别吧：

![](/aitool/picture/sd-webui/study/238.png)

- MiDaS 深度信息估算

    ControlNet的Depth模式能够生成深度图，帮助用户更好地理解图像中各个元素之间的距离关系。MiDaS深度信息估算是一种常用的预处理器，它能够通过控制空间距离来更好地表达较大纵深的风景的远近关系。此外，Depth模式还可以生成遮罩蒙版，帮助用户更好地控制图像中不同元素的显示效果。

- LeReS 深度信息估算

    LeReS方法的成像焦点位于中间景深层，这使得它能够更好地捕捉中距离物品的边缘细节，并且在处理图像时能够拥有更远的景深。因此，当需要突出中景细节或者对整体深度感有更高要求时，可以选择使用LeReS方法。
    然而，LeReS方法在处理近景图像时可能会产生一些模糊，特别是近景物品的边缘部分。因此，如果近景细节是关键或者画面中有大量的近景内容时，可能需要考虑使用MiDaS方法。

- Zoe深度信息计算

    在ControlNet中，Zoe深度信息计算可以帮助用户更好地理解和处理图像中的深度信息。
    具体来说，Zoe深度信息计算是一种相对深度估计和绝对深度估计相结合的方法，它可以估计图像中每个像素的深度信息。

    通过这种方式，Zoe深度信息计算能够将已有数据集的深度信息转移到新的目标数据集上，从而实现零样本（Zero-shot）深度估计。
    
    在ControlNet中，Zoe深度信息计算可以作为一个预处理器或者一个模块来使用，它可以帮助用户更好地理解图像中的深度信息，从而更好地控制和处理图像生成、分割、增强等任务。
    
    例如，在图像生成任务中，通过结合Zoe深度信息计算，用户可以更好地控制生成图像的深度信息，从而生成更加真实、准确的图像。
    
    此外，Zoe深度信息计算还可以用于图像分割和增强等任务中，帮助用户更好地处理图像中的深度信息，提高处理效果和速度。

- 【MiDaS 深度信息估算】、【LeReS 深度信息估算】和【Zoe深度信息计算】的区别：
    - MiDaS是一种基于神经网络的深度估计方法，旨在从图像中推断出场景中物体的距离信息。它通过Encoder和Decoder两个主要组件来实现从输入图像到深度图的映射。MiDaS的优点包括能够在单个前向传播过程中生成高质量的深度图，并且具有较低的计算成本。相比于传统的基于立体匹配的方法，MiDaS不需要额外的视差图或多视角图像，只需输入单张RGB图像即可完成深度估计，这使得它在实际应用中更加灵活和高效。此外，MiDaS还具有一定的泛化能力，可以处理不同场景和物体的深度估计。
    - LeReS深度信息估算也是用于检测图片的景深和光影的深度估计方法，但相对于MiDaS，其精度更高，出图时间也更长。
    - Zoe深度信息计算是一种结合了度量深度估计和相对深度估计的方法，可以用于估计图像中每个像素的深度信息。它能够将已有数据集的深度信息转移到新的目标数据集上，从而实现零样本（Zero-shot）深度估计。

**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/239.png)

**生成图**

![](/aitool/picture/sd-webui/study/240.png)


### 10.5.3 Normal Map 法线贴图

**1. 原理与配置介绍**

可以体现景深关系的图像叫 NormalMap 法线贴图，要理解它的工作原理，我们需要先回顾下法线的概念。

我们在中学时期有学过法线，它是垂直与平面的一条向量，因此储存了该平面方向和角度等信息。我们通过在物体凹凸表面的每个点上均绘制法线，再将其储存到 RGB 的颜色通道中，其中 R 红色、G 绿色、B 蓝色分别对应了三维场景中 XYX 空间坐标系，这样就能实现通过颜色来反映物体表面的光影效果，而由此得到的纹理图我们将其称为法线贴图。由于法线贴图可以实现在不改变物体真实结构的基础上也能反映光影分布的效果，被广泛应用在 CG 动画渲染和游戏制作等领域。

ControlNet 的 NormalMap 模型就是根据画面中的光影信息，从而模拟出物体表面的凹凸细节，实现准确还原画面内容布局，因此 NormalMap 多用于体现物体表面更加真实的光影细节。使用 NormalMap 模型绘图后画面的光影效果立马有了显著提升。

- Normal Map 法线贴图能根据原始素材生成一张记录凹凸信息的法线贴图，便于AI给图片内容进行更好的光影处理，它比深度模型对于细节的保留更加的精确。法线贴图在游戏制作领域用的较多，常用于贴在低模上模拟高模的复杂光影效果。

- normal bae这个 ControlNet 模型，用在模拟3D模型，表面的细节跟凹凸纹理，可以保留下较精确的角色细节。非常适合换服饰。

- NormalMap 有 Bae 和 Midas2 种预处理器，MiDaS 是早期 v1.0 版本使用的预处理器，官方已表示不再进行维护，平时大家使用默认新的 Bae 预处理器即可.



**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/241.png)

**生成图**

![](/aitool/picture/sd-webui/study/242.png)


可以看出光影效果很好

### 10.5.4 OpenPose（人体骨骼）


**1. 原理与配置介绍**

OpenPose 姿态检测可生成图像中角色动作姿态的骨架图，这个骨架图可用于控制生成角色的姿态动作。

- openpose（”Openpose” = Openpose body）
- openpose_face
- openpose_faceonly
- openpose_full（”Openpose Full” = Openpose body + Openpose hand + Openpose face）
- openpose_hand

**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/243.png)

**生成图**

![](/aitool/picture/sd-webui/study/244.png)

### 10.5.5 MLSD（直线）

**1. 原理与配置介绍**

MLSD Preprocessor 最适合生成强有力的线条，这些线条能够检测出需要独特和刚性轮廓的建筑和其他人造作品。但是它不适用于处理非刚性或弯曲的物体。

MLSD 适用于生成室内布局或建筑结构，因为它可以突出直线和边缘。 

**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/245.png)

**生成图**

![](/aitool/picture/sd-webui/study/246.png)

建组的线条变成了大理石

### 10.5.6 Lineart（艺术线条）

**1. 原理与配置介绍**

- lineart_anime（动漫）
- lineart_anime_denoise（动漫、降噪）
- lineart_coarse（粗略）
- lineart_realistic（写实）
- lineart_standard (from white bg & black line)（标准）
- invert (from white bg & black line)（反转）

Lineart（线稿）通常指的是只包含轮廓线条的图像，没有颜色或阴影。这种风格常用于插画、动画或漫画创作。
提取风格图片的中线条，使用controlNet 一起生成新的图片
大致为动漫、写实、准备三种类型


**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/247.png)

**生成图**

![](/aitool/picture/sd-webui/study/248.png)


### 10.5.7 SoftEdge（软边）

**1. 原理与配置介绍**

使用软边缘图像生成训练，平滑的边缘、更好的深度，忽略内部细节，方便创作更具绘画特征或艺术风格的图片。
SoftEdge可能指的是一种图像处理技术，用于平滑图像的边缘，使其看起来更加柔和，减少锐利或锯齿状的外观。

- 预处理器：从图片中提取软边缘图。


- softedge_hed：合成柔边图。
- softedge_hedsafe：安全的合成柔边图，边缘更清晰，稳定性更高。
- softedge_pidinet：简笔柔边图，描绘的细节更少，出图的细节更自由，姿势更稳定。
- softedge_pidinet：安全的简笔柔边图，边缘更清晰，稳定性更高。



**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/249.png)

**生成图**


图的质量并不好，向窗户这种类型并没有很好的展示出来，像素也不高

![](/aitool/picture/sd-webui/study/250.png)

反而是动画模型，这种风格非常不错

![](/aitool/picture/sd-webui/study/251.png)


### 10.5.8 Scribble/sketch（涂鸦/草图）

**1. 原理与配置介绍**

Scribble和Sketch都指的是手绘或草图风格的图像处理效果。Scribble可能更倾向于随意涂鸦的效果，而Sketch则可能更偏向于整洁的素描效果。

- scribble_hed 
    
    scribble_hed预处理器出图的线条虽然简单，但是能发现scribble模型对线条很敏感，线条的增加减少都会得到很明确的反馈。

- scribble_pidinet
    
    相比hed，pidinet会保持线条尽量连贯，但是会舍弃一些小细节 

- scribble_xdog:
    
    DoG实际上是一种类似canny的边缘检测，xdog改进了dog，加宽了边缘，添加了阈值选项进行范围判定

    - 阈值越小，轮廓边缘越宽，还原的原图内容越多；
    - 阈值越大，边缘会变细一点，但是会舍弃很多内容
- t2ia_sketch_pidi:
    
    t2i系列系列的素描图，线条呈现的更好，但是生成图的时候会出现特定的面部生成不柔和

- invert (from white bg & black line) L提取黑白照片


**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/252.png)

**生成图**

![](/aitool/picture/sd-webui/study/253.png)


### 10.5.9 segmentation / 语义分割

**1. 原理与配置介绍**

它可以将照片上不同种类的东西，用不同的颜色表示出来

这样我们新生成的图片里的元素就会跟原图一模一样

![](/aitool/picture/sd-webui/study/254.png)


**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/255.png)

**生成图**

![](/aitool/picture/sd-webui/study/256.png)

### 10.5.10 shuffle（洗牌）

**1. 原理与配置介绍**

对图像进行随机的旋转、缩放、裁剪等变换，生成同风格的重组图片。相当于旧版style，比旧版更可控。
图中颜色还在 进行重新抽卡


**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/257.png)

**生成图**

![](/aitool/picture/sd-webui/study/258.png)

等于是重新抽卡了，跟原来图有很大区别


### 10.5.11 Tile/Blur

**1. 原理与配置介绍**

Tile可能指的是将图像分割成小块（或瓦片）并进行处理的技术；Blur则是一种常用的图像处理技术，用于使图像变得模糊，减少细节或锐利的边缘。

总的来说，该模型有两种行为：

-   忽略图像中的细节并生成新的细节。
-   如果本地图块语义和提示不匹配，则忽略全局提示，并根据本地上下文引导扩散。


由于该模型可以生成新的细节并忽略现有的图像细节，因此我们可以使用该模型去除不良细节并添加精致的细节。
例如，消除因图像大小调整而导致的模糊。

- blur_gaussian：

    高斯模糊，主要用于调整景深用的

- tile_colorfix: 

    保持图片布局的同时固定图片的颜色

- tile_colorfix+sharp: 

    保持图片布局的同时固定图片的颜色，并做一些锐化

- tile_resample:

    仅保持图片布局，颜色会进行一些变化。



**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/259.png)

**生成图**

细节修复，使用blur_gaussian，白色背景 Sigma 参数不建议大于3,否则都是白点，正常深色还是9

这里重点看一下参数sigma，用于控制图片的模糊程度，参数值越高，出来的图片越模糊。为了便于查看效果，我们固定图片的种子。只调整sigma参数值。

![](/aitool/picture/sd-webui/study/260.png)


`tile_colorfix` `tile_resample` 改变部分样式，不过颜色不再能调整  

variation 参数要调低

改变提示词

<PromptTemplate>
    <template v-slot:content>
        <span>1girl,golden hair,<b class="prompt-mark-01">open clothes,open eyes,purple eyes,</b></span>
    </template>
</PromptTemplate>


![](/aitool/picture/sd-webui/study/261.png)

tile_colorfix+sharp不好用，也不建议用


### 10.5.12 Inpaint（局部重绘）

**1. 原理与配置介绍**

ControlNet Inpaint算法与Stable DIffusion系列模型原生的Inapinting操作一样，使用mask对需要重绘的部分进行遮盖，然后进行局部的图像重新生成。

ControlNet Inpaint模型是用50%随机mask和50%随机光流mask共同训练的。这意味着模型不仅支持常规的图像重绘应用，还可以处理视频光流变形任务。

与此同时，ControlNet Inpaint算法也可以进行扩充重绘（outpainting），比如说将人物半身图片扩充重绘成全身图片，将风景画的内容扩展补充，得到一个更大尺寸的图像。

社交平台上时不时火一阵的AI扩图，其核心技术就是通过ControlNet Inpaint来实现。

目前ControlNet Inpaint算法中包含了三个预处理器，分别是inpaint_global_harmonious预处理器，inpaint_only预处理器以及inpaint_only+lama预处理器。

ControlNet Inpaint模型目前只有control_v11p_sd15_inpaint一种，其对应的配置文件为control_v11p_sd15_inpaint.yaml。


**2. 应用**

- 提示词

<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 配置

![](/aitool/picture/sd-webui/study/262.png)

**生成图**

![](/aitool/picture/sd-webui/study/263.png)

生成脸部局部重绘，其他地方进行扩展的图片


### 10.5.13 InstructP2P

**1. 配置介绍**

InstructP2P 的全称为 Instruct Pix2Pix 指导图生图，使用的是 Instruct Pix2Pix 数据集训练的 ControlNet。

它的功能可以说和图生图基本一样，会直接参考原图的信息特征进行重绘，因此并不需要单独的预处理器即可直接使用。

![](/aitool/picture/sd-webui/study/264.png)

**2. 自动修图的功能**

- 配置相关
    - 不需要提示词
    - 小于0.7与本图无关联
    - 小于1.0一些元素还可以看到
    - 大于1.0 图片保留元素自动修图 


- 权重0.7

![](/aitool/picture/sd-webui/study/265.png)

- 权重1.0

![](/aitool/picture/sd-webui/study/266.png)

- 权重1.1

![](/aitool/picture/sd-webui/study/267.png)

- 权重1.2

![](/aitool/picture/sd-webui/study/268.png)


**3. 使用指令修图**

通过采用指令式提示词,来直接对图片进行指令控制
公式：`make Y into X` ， `make Y (into) X(into可以隐藏)`

```js
//修改特定的人
Make him into Trump
//修改颜色
Make it into pink
//修改风格
Make it into winter style 
//修改雪景
make it snow
//修改火焰场景
make it fire
```

- 将人物修改为dva（Make her into dva）

![](/aitool/picture/sd-webui/study/269.png)

- 粉色空间（Make it into pink）

![](/aitool/picture/sd-webui/study/270.png)

- 修改火焰场景（make it fire）

![](/aitool/picture/sd-webui/study/271.png)

- 修改为雪景1（Make it into winter style ）

![](/aitool/picture/sd-webui/study/272.png)

- 修改为雪景1（Make it snow ）

![](/aitool/picture/sd-webui/study/273.png)


### 10.5.14 Reference

**1. 原理与配置介绍**

Reference模型是一种预处理器，可以根据导入的素材图片，根据图片的配色、色调、画风、画中的事物，创建出新图片，使画中事物仍然存在多样性差异。这个模型在涂鸦或线稿生成等场景中应用较广，能产生与参考图风格类似但细节不同、元素多样的图片。

在具体使用过程中，如果希望得到风格完全和参考图一致的图像，只需要让“完美匹配像素”的选项被选中即可；而如果希望得到的图像风格与参考图有所区别，那么就可以通过适当调整“Style Fidelity”滑动条的值来实现，其参考值越少，生成的图片风格和参考图的差异就越大，反之亦然。另外，在绘制SD基本参数时，不能勾选“高分辨率修复”，否则ControlNet的Reference处理将会失效。


ControlNet的Reference模型提供以下几个预处理器：

- 1.reference_adain（官方文档无相关具体介绍）
    - 它可能用于在进行参考图像处理时，利用 AdaIN (Adaptive Instance Normalization) 机制，实现风格迁移网络(Style Transfer)等类似任务。
    - AdaIN 是一种用于图像样式迁移的方法，它通过将内容图像的条件概率分布与样式图像的边缘概率分布相匹配，实现将样式图像的风格应用于内容图像。
    - 这种机制可以帮助模型更好地实现图像样式的迁移和转换。
- 2.reference_adain+attn
    - ControlNet中Reference模型的Reference_adain+attn预处理器是一种高级的预处理器，用于在基于图像的生成模型（如DALL-E等）中实现精细的图像控制。
    - 它通过将输入图像的参考部分进行高级特征转换，来生成具有参考特征的新图像。
    - 这种预处理器能帮助模型在风格迁移、跨域生成等任务中取得更好的效果
- 3.reference_only
    - ControlNet的Reference_only预处理器是一个功能，允许用户在给定参考图像的情况下生成新的图像。
    - 这个预处理器主要应用于根据图生图的场景，基本上保留原图的风格，但只有一张图或者图很少的情况。
    - 这个方法对于懒于进行复杂操作或没有时间进行大量训练的用户来说是很有用的。
- 4.【reference_adain】、【reference_adain+attn】和【reference_only】的区别
    - 【reference_adain】是一种运用 AdaIN 风格的搬迁算法，它能够使生成的图像更接近模型，对颜色和脸型的改变特别明显，更侧重于任何二次元风格。
    - 【reference_only】则是允许用户在给定参考图像的情况下生成新的图像，基本上保留原图的风格，这种方法对于懒于进行复杂操作或没有时间进行大量训练的用户来说是很有用的。
    - ControlNet中Reference模型的Reference_adain+attn预处理器是一种高级的预处理器，用于在基于图像的生成模型（如DALL-E等）中实现精细的图像控制。它通过将输入图像的参考部分进行高级特征转换，来生成具有参考特征的新图像。这种预处理器能帮助模型在风格迁移、跨域生成等任务中取得更好的效果



**2. 应用**

- 配置

![](/aitool/picture/sd-webui/study/274.png)


- 提示词1

<PromptTemplate>
    <template v-slot:content>
        1girl, solo, dress, brown hair, black dress, window, standing, see-through, black footwear, full body, indoors, high heels, realistic, lips
    </template>
</PromptTemplate>

**生成图**

![](/aitool/picture/sd-webui/study/275.png)

- 提示词2 

**生成图**

![](/aitool/picture/sd-webui/study/276.png)


### 10.5.15 Recolor

**1. 原理与配置介绍**

ControlNet中的Recolor模型用于实现对图像色彩的精准控制。具体来说，它可以根据输入的黑白老照片，自动分析图像中的元素，并将适当的色彩赋予不同元素，从而重新输出图像。这种方法可以帮助用户修复历史老照片，使其焕发生机和活力。

此外，Recolor模型还支持自定义元素的颜色。通过Prompt，用户可以指定人物的皮肤、头发、衣着等元素进行重新上色，以实现更精细的图像控制。

- recolor_intensity

    它可能用于调整图像的色彩强度或对比度，以增强图像的视觉效果或突出图像的某些特征。 recolor_intensity可能通过调整图像的颜色分布、色彩饱和度或对比度等参数来实现对图像色彩的改变和增强。这种预处理方法通常被用于图像增强、特征提取、图像分割等任务中，以改善图像的质量和视觉效果。

- recolor_luminance

    它可能用于在进行色彩重上色时考虑图像的亮度信息，以提高色彩重上色的质量和准确性

- Gamma Correction参数
    
    该选项用于调节图片的明暗，参数值越小图片越亮，参数值越大图片越暗。默认值为1。


**2. 应用**

- 下载

webui直接无法下载， 需要先去下载文件

地址：https://huggingface.co/lllyasviel/sd_control_collection/tree/main

![](/aitool/picture/sd-webui/study/277.png)

- 配置

![](/aitool/picture/sd-webui/study/278.png)

- 给人物特定颜色 
    
    提示词 `blue hair,red dress,`

    ![](/aitool/picture/sd-webui/study/279.png)

- 黑白上色

    黑白图

    ![](/aitool/picture/sd-webui/study/280.png)

    自动上色，裙子认成了紫色

    ![](/aitool/picture/sd-webui/study/281.png)


### 10.5.16 Revision（XL专用）

**1. 原理与配置介绍**

对输入图像进行修订或修改，用于生成修订后的图像。

ControlNet中的revision模型是一个面向图像分割的神经网络模型，其主要功能是对输入的图像进行分割，并对其中的每一个像素点进行分类和定位。具体来说，revision模型采用了一种基于区域分割的策略，从粗糙到精细地逐步完善图像分割的结果。该模型在处理图像分割任务时，能够有效地捕捉到图像中的各种细节信息和特征，从而得到更加准确和细致的分割结果。

revision是一种使用图像来提示SDXL的方法，因此它只对SDXL模型有效，简单地理解就是revision具备prompt的功能，可以单独使用，也可以结合文字prompt附加使用。

- revision_clipvision（官方文档无相关具体介绍）

    它可能用于在进行版本修正时考虑到视觉效果的因素，以提高修正的准确性和视觉效果的质量。

- revision_ignore_prompt（官方文档无相关具体介绍）

    它可能会忽略提示词，忽视提示词对其画面的影响和舒服，自我放飞式生成。

**2. 应用**

- 配置

![](/aitool/picture/sd-webui/study/282.png)

- 提示词
<PromptTemplate>
    <template v-slot:content>
        1girl,red dress,golden hair,castle background,
    </template>
</PromptTemplate>

- 使用 revision_clipvision

![](/aitool/picture/sd-webui/study/283.png)

- 使用 revision_ignore_prompt

![](/aitool/picture/sd-webui/study/284.png)

### 10.5.17 T2I-Adapter

**1. 原理与配置介绍**

T2I-Adapter 由腾讯 ARC 实验室和北大视觉信息智能学习实验室联合研发的一款小型模型，它的作用是为各类文生图模型提供额外的控制引导，同时又不会影响原有模型的拓展和生成能力。名称中 T2I 指的是的 text-to-image，即我们常说的文生图，而 Adapter 是适配器的意思。

T2I-Adapter 被集成在 ControlNet 中作为某一控制类型的选项，但实际上它提供了 Lineart、Depth、Sketch、Segmentation、Openpose 等多个类型的控图模型来使用。

T2I-Adapter 的特点是体积小，参数级只有 77M，但对图像的控制效果已经很好，且就在 9 月 8 日，它们针对 SDXL 训练的控图模型刚刚发布，是目前最推荐用于 SDXL 的 ControlNet 模型，但需要注意的是 SDXL 类模型对硬件要求较高，官方建议至少需要 15GB 的显卡内存，想体验的小伙伴可在下面地址中自行下载安装到本地。
T2I-Adapter 控图模型代码仓库地址： https://huggingface.co/TencentARC/T2I-Adapter/tree/main/models


它的信息必须描述准确，ControllNet不在提供偏移，完全依赖输入的提示词

大小比较低，但是生成的图真的不太准确

**2. 应用**

- 配置

![](/aitool/picture/sd-webui/study/285.png)

- 提示词
<PromptTemplate>
    <template v-slot:content>
       1girl,brown hair,black dress,window,standing,
    </template>
</PromptTemplate>

- T2ia_color_grid （颜色像素格生成图）

    ![](/aitool/picture/sd-webui/study/286.png)

- t2ia_sketch_pidi （涂鸦生成图）

    ![](/aitool/picture/sd-webui/study/287.png)

- t2ia_sketch_pidi （风格生成图）与 Shuffle 一样都是转换风格 

    ![](/aitool/picture/sd-webui/study/288.png)

### 10.5.18 IP-Adapter 图生图适配器

**1. 原理与配置介绍**

IP-Adapter 是腾讯的另一个实验室 Tencent AI Lab 研发的控图模型。名称中的 IP 指的是 Image Prompt 图像提示，它和 T2I-Adapter 一样是一款小型模型，但是主要用来提升文生图模型的图像提示能力。IP-Adapter 自 9 月 8 日发布后收到广泛好评，因为它在使用图生图作为参考时，对画面内容的还原十分惊艳，效果类似 Midjourney 的 V 按钮。它也同样内置了多种控图模型，下图中是官方提供的绘图案例。

IP-Adapter 的参数级比 T2I-Adapter 更小，只有 22MB。IP-Adapter 也发布了针对 SDXL_1.0 的控图模型，感兴趣的小伙伴可以通过下方的链接下载尝试。

IP-Adapter，它的全称是 Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models，翻译成中文就是：用于文本到图像扩散模型的文本兼容图像提示适配器，名字有些长，读起来也很拗口。不过我们可以简单的理解就是图片提示或者垫图。

**2. 应用**

- 下载（https://huggingface.co/h94/IP-Adapter/tree/main）

![](/aitool/picture/sd-webui/study/289.png)

下载这两种

![](/aitool/picture/sd-webui/study/290.png)

- 配置

![](/aitool/picture/sd-webui/study/291.png)


生成不成功不知道原因

**3. 20240318补充**

- 下载
    
    新版本的IP-Adapter 可以使用脸部生成新的数据
    
    模型下载地址：https://huggingface.co/h94/IP-Adapter/tree/main/models

- 配置项

![](/aitool/picture/sd-webui/study/292.png)

**生成图1**

![](/aitool/picture/sd-webui/study/293.png)

**生成图2**

![](/aitool/picture/sd-webui/study/294.png)

## 10.6 多重ControlNet应用

有时候单个ControlNet无法满足需求，下列这些都是多个ControlNet配合使用的

### 10.6.1 画出手在脑袋前后

- 使用ControlNet配置 OpenPose(姿态)+Depth(深度)

- 原图

![](/aitool/picture/sd-webui/study/295.jpg)

- ControlNet 配置1

![](/aitool/picture/sd-webui/study/296.png)

- ControlNet 配置2

![](/aitool/picture/sd-webui/study/297.png)

**生成图**

![](/aitool/picture/sd-webui/study/298.png)

### 10.6.2 高还原原图的组合  

- 使用ControlNet配置  OpenPose(姿态)+Depth(深度)+硬边缘（Canny）

- 原图

![](/aitool/picture/sd-webui/study/235.png)

- ControlNet 配置1

![](/aitool/picture/sd-webui/study/299.png)

- ControlNet 配置2

![](/aitool/picture/sd-webui/study/300.png)

**效果图**

![](/aitool/picture/sd-webui/study/301.png)

跟原本的图相比，除了没有纱与高叉裙，简直完美

- ControlNet 配置3

![](/aitool/picture/sd-webui/study/302.png)

**效果图**

![](/aitool/picture/sd-webui/study/303.png)

加入硬边缘可以解决裙子开叉问题