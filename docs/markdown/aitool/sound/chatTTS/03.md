# 三、chatTTS 学习笔记

## 3.1 文本转语音参数

![](/aitool/sound/chatTTS/008.png)

1. spk_emb: 音色 

- tensor对象  打印出的数据
- tensor([ 5.1532e-01,  ...,  4.2509e+00], device='cuda:0',grad_fn=`<AddBackward0>`)

2. temperatur、 top_P 、top

三个值  是语言类大模型(LLM, Large Language Model)中的常见参数

![](/aitool/sound/chatTTS/009.png)


3. prompt 参数

可以将语速放入其中 `[speed_0]` 到 `[speed_9]` 

## 3.2 文本语气化转换参数

![](/aitool/sound/chatTTS/010.png)

1. oral 口语化程度

`[oral_0]`到`[oral_9]` 值越大 口语化出现的机率越大 像 然后、最后、在什么…之前

2. lanugh 笑声频率

`[lanugh_0]`到`[lanughl_9]` 值越大 `[lanugh]` 标签出现的机率越大

3. break 笑声频率

`[break_0]`到`[break_9]` 值越大 `[uv_break]` 出现的机率越大


## 3.3 插入的标签

1. 停顿标签
    - [uv_break]  短停顿
    - [l_break] 长停顿
2. 笑声标签
    - [langh] 笑声

## 3.4 固定音色

1. 获取

调用speaker 后每次都使用这一种类型

![](/aitool/sound/chatTTS/011.png)

2. 调用

![](/aitool/sound/chatTTS/012.png)


## 3.5 使用 fastapi 提供接口

```py
import io
import os


import ChatTTS
from fastapi import FastAPI, Query
from pydantic import BaseModel
import torch
import uvicorn
import numpy as np
import argparse
from fastapi.responses import StreamingResponse
import wave
from dotenv import load_dotenv

load_dotenv("sha256.env")
from tools.utils.text_split_method import text_split_registry
from zh_normalization import TextNormalizer
import uuid
from pathlib import Path


app = FastAPI()
m = TextNormalizer()


class TTS(BaseModel):
    """TTS GET/POST request"""
    """
      1.text: 输入文字  
      2.Audio temperature(音频采样温度)
            较低值（接近0）会使生成的语音更确定和稳定，调高（接近1）会使生成的语音更具随机性和多样性。
      3.top_P(音频采样概率阈值)
            用于控制生成内容的多样性。
      4.top_K(音频采样词汇率)
            用于调节词汇概率
      5.refineTextFlag（精炼文字状态）
            使用后文字会自由调节，适配与当前音色
      6.audioSeed(音色种子id)
            不同的种子id生成不同
      7.TextSeed(分词种子id)
            不同的种子id生成不同
      8.autoBreakDegree(自动填充停顿 refineTextFlag为true可用)
            用于调节生成音频的停顿程度。比如：会适当的添加停顿，值越高，停顿频率越大。
      9.autoLaughDegree(自动填充笑声 refineTextFlag为true可用)
            用于调节生成音频的笑声程度。比如：会随机在某个地方添加笑声。
      10.autoOralDegree(口语化程度 refineTextFlag为true可用)
            用于调节生成音频的自然程度。比如会添加一些连接词:这个、啊、就，等字，让音频更加自然。
      11.speed (语速)
            用于调节生成音频的总体语速。
    """
    # 输入的声音
    text: str = Query("使用API接口", description="输入文字")
    temperature: float = Query("0.3", description="音频采样温度")
    topP: float = Query(0.7, description="音频采样概率阈值")
    topK: int = Query(20, description="音频采样词汇率")
    refineTextFlag: bool = Query(True, description="精炼文字状态")
    audioSeed: int = Query(2, description="音色种子id")
    textSeed: int = Query(42, description="分词种子id")
    autoBreakDegree: int = Query(6, description="自动填充空格")
    autoLaughDegree: int = Query(0, description="自动填充笑声")
    autoOralDegree: int = Query(2, description="口语化程度")
    speed: int = Query(0, description="语速")


def tts_handle(params: TTS):
    torch.manual_seed(params.audioSeed)
    # 可以使用固定音色  先试试随机是否会出现问题
    rand_spk = chat.sample_random_speaker()
    # 语速参数设置
    speed = '[speed_' + str(params.speed) + ']'
    params_infer_code = {
        'spk_emb': rand_spk,
        'temperature': params.temperature,
        'top_P': params.topP,
        'top_K': params.topK,
        'prompt': speed
    }

    torch.manual_seed(params.textSeed)

    # 判断是否使用分词
    if params.refineTextFlag:
        # 控制口语化程度 笑声 停顿
        oral_data = '[oral_' + str(params.autoOralDegree) + ']'
        laugh_data = '[laugh_' + str(params.autoLaughDegree) + ']'
        break_data = '[break_' + str(params.autoOralDegree) + ']'
        params_refine_text = {'prompt': oral_data + laugh_data + break_data}

        text = chat.infer(params.text,
                          skip_refine_text=False,
                          refine_text_only=True,
                          params_refine_text=params_refine_text,
                          params_infer_code=params_infer_code
                          )
        print(text)
    else:
        text = params.text

    #  获取声音
    wav = chat.infer(text,
                     skip_refine_text=True,
                     params_infer_code=params_infer_code
                     )

    audio_data = np.array(wav[0], dtype=np.float32)
    sample_rate = 24000
    audio_data = (audio_data * 32767).astype(np.int16)
    file_path = 'files/' + str(uuid.uuid4()) + ".wav"
    with wave.open(file_path, "w") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_data.tobytes())

    file_like = open(file_path, mode="rb")

    response = StreamingResponse(file_like, media_type="audio/wav")
    return response


def wave_header_chunk(frame_input=b"", channels=1, sample_width=2, sample_rate=24000):
    wav_buf = io.BytesIO()
    with wave.open(wav_buf, "wb") as vfout:
        vfout.setnchannels(channels)
        vfout.setsampwidth(sample_width)
        vfout.setframerate(sample_rate)
        vfout.writeframes(frame_input)
    wav_buf.seek(0)
    return wav_buf.read()


def infer(texts, param_code, param_text):
    yield wave_header_chunk()
    for text in texts:
        audio_data = chat.infer(text,
                                skip_refine_text=True,
                                params_infer_code=param_code,
                                params_refine_text=param_text,
                                do_text_normalization=False)[0]
        audio_data = audio_data / np.max(np.abs(audio_data))
        chunks = (audio_data * 32768).astype(np.int16)
        for chunk in chunks:
            if chunk is not None:
                chunk = chunk.tobytes()
                yield chunk


def tts_handle2(params: TTS):
    torch.manual_seed(params.audioSeed)
    rand_spk = chat.sample_random_speaker()
    speed = '[speed_' + str(params.speed) + ']'
    params_infer_code = {
        'spk_emb': rand_spk,
        'temperature': params.temperature,
        'top_P': params.topP,
        'top_K': params.topK,
        'prompt': speed
    }
    # 语速参数设置

    params_refine_text = {}
    torch.manual_seed(params.textSeed)

    # 判断是否使用分词
    texts = text_split_registry["cut2"](params.text)
    # texts = params.text
    if params.refineTextFlag:
        oralStr = '[oral_' + str(params.autoOralDegree) + ']'
        laughStr = '[laugh_' + str(params.autoLaughDegree) + ']'
        breakStr = '[laugh_' + str(params.autoOralDegree) + ']'

        params_refine_text = {'prompt': oralStr + laughStr + breakStr + speed}
        texts = chat.infer(texts,
                           skip_refine_text=False,
                           refine_text_only=True,
                           params_refine_text=params_refine_text,
                           params_infer_code=params_infer_code
                           )
    else:
        texts = texts

    wavs = infer(texts, params_infer_code, params_refine_text)

    return StreamingResponse(wavs, media_type="audio/wav")


# 清除文件夹
def clear_wav(directory):
    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]

    if not files:
        return False, "wavs目录内无wav文件"

    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
                print(f"已删除文件: {file_path}")
            elif os.path.isdir(file_path):
                print(f"跳过文件夹: {file_path}")
        except Exception as e:
            print(f"文件删除错误 {file_path}, 报错信息: {e}")
            return False, str(e)
    return True, "所有wav文件已被删除."


# 版本1 直接使用原本的
@app.post("/v1/txt2voice")
async def index_post(params: TTS):
    return tts_handle(params)


# 版本2 处理手机号与数字 新版本不知道是否需要
@app.post("/v2/txt2voice")
def index_post(params: TTS):
    tempText = params.text
    temps = m.normalize(tempText)
    params.text = ''.join(temps)
    return tts_handle(params)


# 版本3  解决长文本问题
# @app.post("/v3/txt2voice")
# def index_post(params: TTS):
#     return tts_handle2(params)

# 根据编号获取固定音色
@app.get("/speaker/{audio_seed}")
async def read_item(audio_seed: int):
    torch.manual_seed(audio_seed)
    rand_spk = chat.sample_random_speaker()
    return rand_spk

# 清理文件
@app.get('/clear_wavs')
def clear_wavs():
    dir_path = './files'  # wav音频文件存储目录
    success, message = clear_wav(dir_path)
    if success:
        return 1
    else:
        return 0

# 对当前词进行语气化处理
@app.post("/convert_text")
async def read_item(params: TTS):
    torch.manual_seed(params.textSeed)
    oral_data = '[oral_' + str(params.autoOralDegree) + ']'
    laugh_data = '[laugh_' + str(params.autoLaughDegree) + ']'
    break_data = '[break_' + str(params.autoOralDegree) + ']'
    params_refine_text = {'prompt': oral_data + laugh_data + break_data}

    text = chat.infer(params.text,
                      skip_refine_text=False,
                      refine_text_only=True,
                      params_refine_text=params_refine_text
                      )
    return text


# 对当前词进行语气化处理
@app.post("/text_convert_audio")
async def read_item(params: TTS):
    print("使用的参数类型：  "+str(params))

    torch.manual_seed(params.audioSeed)
    # 可以使用固定音色  先试试随机是否会出现问题
    rand_spk = chat.sample_random_speaker()
    torch.manual_seed(params.textSeed)
    # 语速参数设置
    speed = '[speed_' + str(params.speed) + ']'
    params_infer_code = {
        'spk_emb': rand_spk,
        'temperature': params.temperature,
        'top_P': params.topP,
        'top_K': params.topK,
        'prompt': speed
    }
    #  获取声音
    wav = chat.infer(params.text,
                     skip_refine_text=True,
                     params_infer_code=params_infer_code
                     )

    audio_data = np.array(wav[0], dtype=np.float32)
    sample_rate = 24000
    audio_data = (audio_data * 32767).astype(np.int16)
    file_path = 'files/' + str(uuid.uuid4()) + ".wav"
    with wave.open(file_path, "w") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_data.tobytes())

    # return FileResponse(file_path)
    # file_like = open(file_path, mode="rb")
    # response = StreamingResponse(file_like, media_type="audio/wav")
    # return response
    file_path = Path(file_path)

    return StreamingResponse(file_path.open("rb"), media_type="audio/x-wav")

if __name__ == '__main__':
    global chat
    chat = ChatTTS.Chat()

    parser = argparse.ArgumentParser(description='ChatTTS demo Launch')
    parser.add_argument("--host", type=str, default="0.0.0.0")
    parser.add_argument("--port", type=int, default=7863)
    parser.add_argument('--local_path', type=str, default=None, help='the local_path if need')
    args = parser.parse_args()

    print("loading ChatTTS model...")

    if args.local_path is None:
        chat.load_models()
    else:
        print('local model path:', args.local_path)
        chat.load_models('local', local_path=args.local_path)

    uvicorn.run(app, host=args.host, port=args.port)
```
